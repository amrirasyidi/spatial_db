{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "# import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count_distinct, broadcast, expr\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.captureWarnings(True)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "os.environ[\"POLARS_NUM_THREADS\"] = str(num_cores//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Sedona App\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .config(\"spark.driver.memory\", \"15g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDEFINED VARIABLES\n",
    "ORIGINAL_CRS = 4326\n",
    "PROJECTED_CRS = 3857  # WGS84 Web Mercator (Auxiliary Sphere)\n",
    "BUFFER_RADIUS = 10  # in meter\n",
    "\n",
    "# People movement\n",
    "START_DATE = datetime(2023, 1, 24, 7, 0, 0)  # year, month, date, GMT+7\n",
    "END_DATE = datetime(2023, 2, 23, 7, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kantor Cabang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- No_: string (nullable = true)\n",
      " |-- Nama: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kc = spark.\\\n",
    "    read.\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    csv(\"kantorcabang.csv\")\n",
    "\n",
    "kc.createOrReplaceTempView(\"kc\")\n",
    "kc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| no|                Nama|      kc_buffer_geom|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|Pasar Induk Krama...|POLYGON ((-6.2952...|\n",
      "|  2|Bursa Efek Jakart...|POLYGON ((-6.2233...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kc_buffer = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        CAST(No_ AS INT) AS no,\n",
    "        Nama,\n",
    "        ST_Transform(\n",
    "            ST_Buffer(\n",
    "                ST_Transform(\n",
    "                    ST_POINT(Latitude, Longitude)\n",
    "                    , 'EPSG:{ORIGINAL_CRS}', 'EPSG:{PROJECTED_CRS}'\n",
    "                ), \n",
    "                5000\n",
    "            )\n",
    "            , 'EPSG:{PROJECTED_CRS}', 'EPSG:{ORIGINAL_CRS}'\n",
    "        ) AS kc_buffer_geom\n",
    "    FROM\n",
    "        kc\n",
    "    \"\"\"\n",
    ")\n",
    "kc_buffer.createOrReplaceTempView(\"kc_buffer\")\n",
    "kc_buffer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PlaceID: string (nullable = true)\n",
      " |-- Official_Name: string (nullable = true)\n",
      " |-- Primary_Category_ID: string (nullable = true)\n",
      " |-- Primary_Category_Name: string (nullable = true)\n",
      " |-- Primary_Food_Type_Name: string (nullable = true)\n",
      " |-- Chain_Name: string (nullable = true)\n",
      " |-- House_Number: string (nullable = true)\n",
      " |-- Full_Street_Name: string (nullable = true)\n",
      " |-- Admin_Level_2: string (nullable = true)\n",
      " |-- Admin_Level_3: string (nullable = true)\n",
      " |-- Admin_Level_4: string (nullable = true)\n",
      " |-- Postal_Code: double (nullable = true)\n",
      " |-- Display_Latitude: double (nullable = true)\n",
      " |-- Display_Longitude: double (nullable = true)\n",
      " |-- Phone: double (nullable = true)\n",
      " |-- TollFree_Phone: string (nullable = true)\n",
      " |-- Mobile_Phone: double (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Credit_CardAccepted: string (nullable = true)\n",
      " |-- Credit_Card_Type: string (nullable = true)\n",
      " |-- Place_Description_Text: string (nullable = true)\n",
      " |-- geometry: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poi = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .parquet(r\"path\\to\\*poi.parquet\")\n",
    ")\n",
    "\n",
    "poi.createOrReplaceTempView(\"poi\")\n",
    "poi.printSchema()\n",
    "# poi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- poi_buffer_geom: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poi_buffer = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    PlaceID as place_id,\n",
    "    Official_Name as official_name,\n",
    "    Display_Longitude as longitude, \n",
    "    Display_Latitude as latitude, \n",
    "    ST_FlipCoordinates(ST_Transform(\n",
    "        ST_Buffer(\n",
    "            ST_Transform(\n",
    "                ST_FlipCoordinates(ST_POINT(Display_Longitude, Display_Latitude))\n",
    "                , 'EPSG:{ORIGINAL_CRS}', 'EPSG:{PROJECTED_CRS}'\n",
    "            ), \n",
    "            {BUFFER_RADIUS}\n",
    "        )\n",
    "        , 'EPSG:{PROJECTED_CRS}', 'EPSG:{ORIGINAL_CRS}'\n",
    "    )) AS poi_buffer_geom\n",
    "FROM\n",
    "    poi\n",
    "\"\"\")\n",
    "poi_buffer.createOrReplaceTempView(\"poi_buffer\")\n",
    "poi_buffer.printSchema()\n",
    "# poi_buffer.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OID_: long (nullable = true)\n",
      " |-- Hashed_Device_ID: string (nullable = true)\n",
      " |-- Lat_of_Observation_Point: double (nullable = true)\n",
      " |-- Local_Date_of_Observation_Point: string (nullable = true)\n",
      " |-- Local_Day_of_Week_of_Observation_Point: string (nullable = true)\n",
      " |-- Local_Time_of_Day_of_Observation_Point: string (nullable = true)\n",
      " |-- Local_Timezone_of_Observation_Point: string (nullable = true)\n",
      " |-- Lon_of_Observation_Point: double (nullable = true)\n",
      " |-- Polygon_ID: string (nullable = true)\n",
      " |-- Unix_Timestamp_of_Observation_Point: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .parquet(r\"path\\to\\*_pm_raw.parquet\")\n",
    ")\n",
    "\n",
    "pm.createOrReplaceTempView(\"pm\")\n",
    "pm.printSchema()\n",
    "# pm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_g = spark.sql(f\"\"\"--sql\n",
    "SELECT\n",
    "    Hashed_Device_ID AS device_id,\n",
    "    CAST(Local_Date_of_Observation_Point AS Date) AS obs_date,\n",
    "    ST_POINT(Lon_of_Observation_Point, Lat_of_Observation_Point, {ORIGINAL_CRS}) AS pm_coordinate\n",
    "FROM\n",
    "    pm\n",
    "WHERE\n",
    "    TRUE\n",
    "    AND Unix_Timestamp_of_Observation_Point BETWEEN {int(START_DATE.timestamp())} AND {int(END_DATE.timestamp())};\n",
    "\"\"\")\n",
    "pm_g.createOrReplaceTempView(\"pm_g\")\n",
    "# pm_g.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Footfall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straigforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- obs_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spatial join between poi and people movement data\n",
    "spatial_join = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    poi_buffer.place_id,\n",
    "    poi_buffer.official_name,\n",
    "    poi_buffer.longitude,\n",
    "    poi_buffer.latitude,\n",
    "    pm_g.device_id,\n",
    "    pm_g.obs_date\n",
    "FROM\n",
    "    poi_buffer, pm_g\n",
    "WHERE\n",
    "    TRUE\n",
    "    AND ST_Contains(poi_buffer.poi_buffer_geom, pm_g.pm_coordinate)\n",
    "\"\"\")\n",
    "spatial_join.createOrReplaceTempView(\"spatial_join\")\n",
    "spatial_join.printSchema()\n",
    "# spatial_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- num_of_visit: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last run: 5 mins\n",
    "footfall_count = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT place_id,\n",
    "        official_name,\n",
    "        COUNT(DISTINCT device_id) as num_of_visit\n",
    "    FROM \n",
    "        spatial_join\n",
    "    GROUP BY\n",
    "        1,2\n",
    "    ORDER BY\n",
    "        place_id\n",
    "    \"\"\"\n",
    ")\n",
    "footfall_count.printSchema()\n",
    "# footfall_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to parquet, can also convert to pandas first\n",
    "# footfall_count.repartition(1).write.format(\"parquet\").mode(\"append\").save(\"footfall.parquet\")\n",
    "footfall_count_df = footfall_count.toPandas()#.to_parquet(\"footfall.parquet\")\n",
    "footfall_count_df.to_parquet('result/temp.parquet')\n",
    "footfall_count_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- poi_buffer_geom: geometry (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"result/poi_buffer_geohashed.parquet\"):\n",
    "    poi_buffer_geohashed = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "        , ST_GeoHash(poi_buffer_geom, 33) as geohash\n",
    "    FROM\n",
    "        poi_buffer\n",
    "    ORDER BY\n",
    "    geohash\n",
    "    \"\"\")\n",
    "    poi_buffer_geohashed.write.format(\"geoparquet\").save(\"result\" + \"/poi_buffer_geohashed.parquet\")\n",
    "else:\n",
    "    poi_buffer_geohashed = (\n",
    "        spark\n",
    "        .read\n",
    "        .option(\"header\", \"true\")\n",
    "        .parquet(\"result/poi_buffer_geohashed.parquet\")\n",
    "    )\n",
    "\n",
    "poi_buffer_geohashed.createOrReplaceTempView(\"poi_buffer_geohashed\")\n",
    "poi_buffer_geohashed.printSchema()\n",
    "# poi_buffer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- obs_date: date (nullable = true)\n",
      " |-- pm_coordinate: geometry (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"result/pm_geohashed.parquet\"):\n",
    "    pm_geohashed = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "        , ST_GeoHash(pm_coordinate, 33) as geohash\n",
    "    FROM\n",
    "        pm_g\n",
    "    ORDER BY\n",
    "        geohash\n",
    "    \"\"\")\n",
    "    pm_geohashed.write.format(\"geoparquet\").save(\"result\" + \"/pm_geohashed.parquet\")\n",
    "else:\n",
    "    pm_geohashed = (\n",
    "        spark\n",
    "        .read\n",
    "        .option(\"header\", \"true\")\n",
    "        .parquet(\"result/pm_geohashed.parquet\")\n",
    "    )\n",
    "\n",
    "pm_geohashed.createOrReplaceTempView(\"pm_geohashed\")\n",
    "pm_geohashed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- obs_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"result/spatial_join.parquet\"):\n",
    "    spatial_join = (\n",
    "        poi_buffer_geohashed.alias(\"poi_buffer\")\n",
    "        .join(\n",
    "            pm_geohashed.alias(\"pm_g\"),\n",
    "            expr(\"ST_Contains(poi_buffer.poi_buffer_geom, pm_g.pm_coordinate)\")\n",
    "        ).select(\n",
    "            \"poi_buffer.place_id\",\n",
    "            \"poi_buffer.official_name\",\n",
    "            \"poi_buffer.longitude\",\n",
    "            \"poi_buffer.latitude\",\n",
    "            \"poi_buffer.geohash\",\n",
    "            \"pm_g.device_id\",\n",
    "            \"pm_g.obs_date\"\n",
    "        ).orderBy(\n",
    "            \"poi_buffer.geohash\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    spatial_join = (\n",
    "        spark\n",
    "        .read\n",
    "        .option(\"header\", \"true\")\n",
    "        .parquet(\"result/spatial_join.parquet\")\n",
    "    )\n",
    "\n",
    "# spatial_join.write.format(\"geoparquet\").save(\"result\" + \"/spatial_join.parquet\")\n",
    "spatial_join.printSchema()\n",
    "# spatial_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the equivalent PySpark operations\n",
    "footfall_count = (\n",
    "    spatial_join.select(\"place_id\", \"official_name\", \"device_id\")\n",
    "    .groupBy(\"place_id\", \"official_name\")\n",
    "    .agg(count_distinct(\"device_id\").alias(\"num_of_visit\"))\n",
    "    .orderBy(\"num_of_visit\")\n",
    ")\n",
    "\n",
    "footfall_count.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_join = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .parquet(\"result/spatial_join.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_spatial_join_df = pl.read_parquet(\"result/spatial_join.parquet/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footfall_count = polars_spatial_join_df.groupby([\"place_id\"]).agg(pl.col(\"device_id\").n_unique().alias(\"unique_device_count\"))\n",
    "footfall_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spatial join between poi and people movement data\n",
    "# spatial_join = spark.sql(\"\"\"\n",
    "# SELECT\n",
    "#     poi_buffer.place_id,\n",
    "#     poi_buffer.official_name,\n",
    "#     poi_buffer.longitude,\n",
    "#     poi_buffer.latitude,\n",
    "#     pm_g.device_id,\n",
    "#     pm_g.obs_date\n",
    "# FROM\n",
    "#     poi_buffer, pm_g\n",
    "# WHERE\n",
    "#     TRUE\n",
    "#     AND ST_Contains(poi_buffer.poi_buffer_geom, pm_g.pm_coordinate)\n",
    "# \"\"\")\n",
    "# spatial_join.createOrReplaceTempView(\"spatial_join\")\n",
    "# spatial_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_join.write.format(\"parquet\").mode(\"overwrite\").save(\"result/spatial_join.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the equivalent PySpark operations\n",
    "# footfall_count = (\n",
    "#     spatial_join.select(\"place_id\", \"official_name\", \"device_id\")\n",
    "#     .groupBy(\"place_id\", \"official_name\")\n",
    "#     .agg(count_distinct(\"device_id\").alias(\"num_of_visit\"))\n",
    "#     .orderBy(\"place_id\")\n",
    "# )\n",
    "\n",
    "# # footfall_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_buffer.alias(\"poi_buffer\").join(broadcast(pm_g).alias(\"pm_g\"), expr(\"ST_Contains(poi_buffer.poi_buffer_geom, pm_g.pm_coordinate)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm_g.write.format(\"geoparquet\").save(\"result\" + \"/pm_geohashed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to csv\n",
    "# footfall_count.write.mode(\"overwrite\").csv(\"footfall.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to shp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
